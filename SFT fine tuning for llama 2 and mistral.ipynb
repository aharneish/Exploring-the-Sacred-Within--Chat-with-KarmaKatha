{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install trl transformers[torch] datasets==2.16.0\n#!git clone https://github.com/huggingface/trl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(\"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\",write_permission=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:46:06.098194Z","iopub.execute_input":"2024-03-06T07:46:06.099162Z","iopub.status.idle":"2024-03-06T07:46:06.760160Z","shell.execute_reply.started":"2024-03-06T07:46:06.099125Z","shell.execute_reply":"2024-03-06T07:46:06.759050Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install bitsandbytes accelerate peft","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig\nimport torch\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer,TrainingArguments\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, DataCollatorForCompletionOnlyLM\nfrom peft import LoraConfig\n\ndataset = load_dataset(\"Aharneish/sprit-qa-t\", split=\"train[:90%]\")\neval_data = load_dataset(\"Aharneish/sprit-qa-t\", split=\"test\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\",load_in_8bit=True,device_map='auto')\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\n#collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n\npeft_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias=\"none\",\n    use_dora=True,\n    task_type=\"CAUSAL_LM\",\n)\ntraining_args = TrainingArguments(\n    output_dir=\"gemma-sprit-test\",\n    learning_rate=1e-5,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    save_strategy=\"steps\",\n    evaluation_strategy=\"steps\",\n    save_steps=500,\n    save_total_limit=2,\n    #load_best_model_at_end=True,\n    fp16=True,\n    push_to_hub=True,\n    report_to='none'\n)\ntrainer = SFTTrainer(\n    model,\n    train_dataset=dataset,\n    eval_dataset=eval_data,\n    dataset_text_field=\"text\",\n    #data_collator=collator,\n    #packing=True,\n    peft_config=peft_config,\n    args = training_args,\n)\n\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:50:39.429317Z","iopub.execute_input":"2024-03-06T07:50:39.430058Z","iopub.status.idle":"2024-03-06T07:51:51.833241Z","shell.execute_reply.started":"2024-03-06T07:50:39.430028Z","shell.execute_reply":"2024-03-06T07:51:51.831878Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b81d380a991c46d5ab317ca1a3b85773"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 37\u001b[0m\n\u001b[1;32m     13\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m     14\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m     15\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAUSAL_LM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     22\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma-sprit-test\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     36\u001b[0m )\n\u001b[0;32m---> 37\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#data_collator=collator,\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#packing=True,\u001b[39;49;00m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:212\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m             output\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    210\u001b[0m         model\u001b[38;5;241m.\u001b[39mget_input_embeddings()\u001b[38;5;241m.\u001b[39mregister_forward_hook(make_inputs_require_grad)\n\u001b[0;32m--> 212\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m args\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_loaded_in_4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    214\u001b[0m     peft_module_casting_to_bf16(model)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/mapping.py:136\u001b[0m, in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config, adapter_name, mixed)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m    135\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m _prepare_prompt_learning_config(peft_config, model_config)\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:1059\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, peft_config: PeftConfig, adapter_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1059\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:126\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_peft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_MODEL_MAPPING[peft_config\u001b[38;5;241m.\u001b[39mpeft_type]\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_gradient_checkpointing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/model.py:111\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:147\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\u001b[38;5;241m.\u001b[39mupdate(peft_config)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter \u001b[38;5;241m=\u001b[39m adapter_name\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:302\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[0;34m(self, model, adapter_name)\u001b[0m\n\u001b[1;32m    300\u001b[0m     is_target_modules_in_base_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     parent, target, target_name \u001b[38;5;241m=\u001b[39m _get_submodules(model, key)\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_and_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_modules_in_base_model:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget modules \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config\u001b[38;5;241m.\u001b[39mtarget_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the base model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the target modules and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/model.py:182\u001b[0m, in \u001b[0;36mLoraModel._create_and_replace\u001b[0;34m(self, lora_config, adapter_name, target, target_name, parent, current_key)\u001b[0m\n\u001b[1;32m    172\u001b[0m     target\u001b[38;5;241m.\u001b[39mupdate_layer(\n\u001b[1;32m    173\u001b[0m         adapter_name,\n\u001b[1;32m    174\u001b[0m         r,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m         use_dora\u001b[38;5;241m=\u001b[39mlora_config\u001b[38;5;241m.\u001b[39muse_dora,\n\u001b[1;32m    180\u001b[0m     )\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m     new_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m adapter_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter:\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;66;03m# adding an additional adapter: it is not automatically trainable\u001b[39;00m\n\u001b[1;32m    185\u001b[0m         new_module\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/model.py:257\u001b[0m, in \u001b[0;36mLoraModel._create_new_module\u001b[0;34m(lora_config, adapter_name, target, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m new_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dispatcher \u001b[38;5;129;01min\u001b[39;00m dispatchers:\n\u001b[0;32m--> 257\u001b[0m     new_module \u001b[38;5;241m=\u001b[39m \u001b[43mdispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# first match wins\u001b[39;00m\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:213\u001b[0m, in \u001b[0;36mdispatch_bnb_8bit\u001b[0;34m(target, adapter_name, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     eightbit_kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    205\u001b[0m     eightbit_kwargs\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    206\u001b[0m         {\n\u001b[1;32m    207\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_fp16_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m: target\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mhas_fp16_weights,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m         }\n\u001b[1;32m    212\u001b[0m     )\n\u001b[0;32m--> 213\u001b[0m     new_module \u001b[38;5;241m=\u001b[39m \u001b[43mLinear8bitLt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meightbit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_module\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:48\u001b[0m, in \u001b[0;36mLinear8bitLt.__init__\u001b[0;34m(self, base_layer, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights, use_rslora, use_dora, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m LoraLayer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, base_layer)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_dora:\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not support DoRA yet, please set it to False\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_active_adapter \u001b[38;5;241m=\u001b[39m adapter_name\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_layer(\n\u001b[1;32m     52\u001b[0m     adapter_name,\n\u001b[1;32m     53\u001b[0m     r,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     use_dora\u001b[38;5;241m=\u001b[39muse_dora,\n\u001b[1;32m     59\u001b[0m )\n","\u001b[0;31mValueError\u001b[0m: Linear8bitLt does not support DoRA yet, please set it to False"],"ename":"ValueError","evalue":"Linear8bitLt does not support DoRA yet, please set it to False","output_type":"error"}]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:47:32.309278Z","iopub.status.idle":"2024-03-06T07:47:32.309729Z","shell.execute_reply.started":"2024-03-06T07:47:32.309499Z","shell.execute_reply":"2024-03-06T07:47:32.309517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from transformers import AutoModelForCausalLM, AutoTokenizer,TrainingArguments\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, DataCollatorForCompletionOnlyLM\nfrom peft import LoraConfig\n\ndataset = load_dataset(\"Aharneish/sprit-qa-t\", split=\"train[:90%]\")\neval_data = load_dataset(\"Aharneish/sprit-qa-t\", split=\"test\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\",load_in_4bit=True,device_map='auto')\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n#collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\ntraining_args = TrainingArguments(\n    output_dir=\"llama-2-chat-sprit-2\",\n    learning_rate=1e-5,\n    num_train_epochs=15,\n    weight_decay=0.01,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    save_strategy=\"steps\",\n    evaluation_strategy=\"steps\",\n    save_steps=500,\n    save_total_limit=2,\n    #load_best_model_at_end=True,\n    fp16=True,\n    push_to_hub=True,\n    report_to='none'\n)\ntrainer = SFTTrainer(\n    model,\n    train_dataset=dataset,\n    eval_dataset=eval_data,\n    dataset_text_field=\"text\",\n    #data_collator=collator,\n    #packing=True,\n    peft_config=peft_config,\n    args = training_args,\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T10:38:19.913830Z","iopub.execute_input":"2024-02-29T10:38:19.914229Z","iopub.status.idle":"2024-02-29T12:18:30.656487Z","shell.execute_reply.started":"2024-02-29T10:38:19.914199Z","shell.execute_reply":"2024-02-29T12:18:30.655506Z"}}},{"cell_type":"markdown","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T12:18:30.658339Z","iopub.execute_input":"2024-02-29T12:18:30.659075Z","iopub.status.idle":"2024-02-29T12:18:33.011584Z","shell.execute_reply.started":"2024-02-29T12:18:30.659045Z","shell.execute_reply":"2024-02-29T12:18:33.010371Z"}}},{"cell_type":"markdown","source":"from transformers import AutoModelForCausalLM,AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2024-02-29T14:25:50.198364Z","iopub.execute_input":"2024-02-29T14:25:50.198684Z","iopub.status.idle":"2024-02-29T14:25:59.216336Z","shell.execute_reply.started":"2024-02-29T14:25:50.198661Z","shell.execute_reply":"2024-02-29T14:25:59.215353Z"}}},{"cell_type":"markdown","source":"model1=AutoModelForCausalLM.from_pretrained(\"/kaggle/working/mistral-test\")\ntokenizer=AutoTokenizer.from_pretrained(\"/kaggle/working/mistral-test\")","metadata":{"execution":{"iopub.status.busy":"2024-02-29T14:25:59.218600Z","iopub.execute_input":"2024-02-29T14:25:59.219207Z","iopub.status.idle":"2024-02-29T14:27:45.600138Z","shell.execute_reply.started":"2024-02-29T14:25:59.219152Z","shell.execute_reply":"2024-02-29T14:27:45.598425Z"}}},{"cell_type":"markdown","source":"model=AutoModelForCausalLM.from_pretrained(\"Aharneish/mistral-test_1\")\ntokenizer=AutoTokenizer.from_pretrained(\"Aharneish/mistral-test_1\")","metadata":{"execution":{"iopub.status.busy":"2024-02-29T15:25:54.928474Z","iopub.execute_input":"2024-02-29T15:25:54.929247Z"}}},{"cell_type":"markdown","source":"from transformers import AutoModelForCausalLM, AutoTokenizer,TrainingArguments\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, DataCollatorForCompletionOnlyLM\nfrom peft import LoraConfig\n\ndataset = load_dataset(\"Aharneish/sprit-qa-t\", split=\"train[:90%]\")\neval_data = load_dataset(\"Aharneish/sprit-qa-t\", split=\"test\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\",load_in_4bit=True,device_map='auto')\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n#collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\ntraining_args = TrainingArguments(\n    output_dir=\"mistral-test_2\",\n    learning_rate=1e-5,\n    num_train_epochs=10,\n    weight_decay=0.01,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    save_strategy=\"steps\",\n    evaluation_strategy=\"steps\",\n    save_steps=500,\n    save_total_limit=2,\n    #load_best_model_at_end=True,\n    fp16=True,\n    push_to_hub=True,\n    report_to='none'\n)\ntrainer = SFTTrainer(\n    model,\n    train_dataset=dataset,\n    eval_dataset=eval_data,\n    dataset_text_field=\"text\",\n    #data_collator=collator,\n    #packing=True,\n    peft_config=peft_config,\n    args = training_args,\n)\n\ntrainer.train()\ntrainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T14:28:00.446283Z","iopub.execute_input":"2024-02-29T14:28:00.447120Z","iopub.status.idle":"2024-02-29T15:25:47.236688Z","shell.execute_reply.started":"2024-02-29T14:28:00.447086Z","shell.execute_reply":"2024-02-29T15:25:47.228137Z"}}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}