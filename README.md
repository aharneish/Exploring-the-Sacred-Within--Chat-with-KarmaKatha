# codes

This reposotory contains all the codes used for fine tuning and rag implementation of the models that are OpenAI's GPT2 model, Meta's llama-2 model, MistralAI's Mistral-v0.1 model.
This repo also contains the dataset along with the collected data files.
# Results

![image](https://github.com/aharneish/Exploring-the-Sacred-Within--Chat-with-KarmaKatha/assets/99192645/2a80a329-3e5a-4208-a381-a75cb95904d1)

# Examples

![image](https://github.com/aharneish/Exploring-the-Sacred-Within--Chat-with-KarmaKatha/assets/99192645/1f5ca3bd-6b14-4d00-846b-0421b4368c7b)
![image](https://github.com/aharneish/Exploring-the-Sacred-Within--Chat-with-KarmaKatha/assets/99192645/6c51a78d-a95c-4614-97fa-02a9d4a9cade)
![image](https://github.com/aharneish/Exploring-the-Sacred-Within--Chat-with-KarmaKatha/assets/99192645/2f69e338-932b-45f0-8888-0604aa3a36a6)

# References
1. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of Deep
Bidirectional Transformers for Language Understanding,” Oct. 11, 2018.
https://arxiv.org/abs/1810.04805
2. A. Vaswani et al., “Attention Is All You Need,” Jun. 12, 2017.
https://arxiv.org/abs/1706.03762
3. Mikolov, Tomas, et al. "Efficient estimation of word representations in vector space."
arXiv preprint arXiv:1301.3781 (2013). https://arxiv.org/abs/1301.3781
4. Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. "Glove: Global
vectors for word representation." Proceedings of the 2014 conference on empirical
methods in natural language processing (EMNLP). 2014.
https://aclanthology.org/D14-1162.pdf
5. Touvron, Hugo, et al. "Llama 2: Open foundation and fine-tuned chat models." arXiv
preprint arXiv:2307.09288 (2023). https://arxiv.org/abs/2307.09288
6. Radford, Alec, et al. "Language models are unsupervised multitask learners." OpenAI
blog 1.8 (2019):9. Language Models are Unsupervised Multitask Learners
7. Tarján, Balázs & Fegyó, Tibor & Mihajlik, Péter. (2022). Morphology aware data
augmentation with neural language models for online hybrid ASR. Acta Linguistica
Academica. 69. 10.1556/2062.2022.00582.
8. Jiang, Albert Q., et al. "Mistral 7B." arXiv preprint arXiv:2310.06825 (2023).
https://arxiv.org/abs/2310.06825
9. Hu, Edward J., et al. "Lora: Low-rank adaptation of large language models." arXiv
preprint arXiv:2106.09685 (2021). https://arxiv.org/abs/2106.09685
10. Brown, Tom, et al. "Language models are few-shot learners." Advances in neural
information processing systems 33 (2020): 1877-1901. https://arxiv.org/abs/2005.14165
11. Xu, Lingling, et al. "Parameter-efficient fine-tuning methods for pretrained language
models: A critical review and assessment." arXiv preprint arXiv:2312.12148 (2023).
https://arxiv.org/abs/2312.12148
12. Lewis, Patrick, et al. "Retrieval-augmented generation for knowledge-intensive nlp
tasks." Advances in Neural Information Processing Systems 33 (2020): 9459-9474.
https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-A
bstract.html
13. FAISS https://faiss.ai/index.html
14. RAGAS https://docs.ragas.io/en/stable/
15. Langchain https://www.langchain.com/
16. https://towardsdatascience.com/beautifully-illustrated-nlp-models-from-rnn-to-transform
er-80d69faf2109
17. https://towardsdatascience.com/evaluating-rag-applications-with-ragas-81d67b0ee31a
18. https://docs.ragas.io/en/latest/concepts/metrics/index.html
19. https://deci.ai/blog/evaluating-rag-pipelines-using-langchain-and-ragas/
20. https://arize.com/blog/ragas-how-to-evaluate-rag-pipeline-phoenix
43
21. https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/
22. https://medium.aiplanet.com/evaluate-rag-pipeline-using-ragas-fbdd8dd466c1
23. https://langfuse.com/guides/cookbook/evaluation_of_rag_with_ragas
24. https://aclanthology.org/2023.tacl-1.1.pdf
25. https://blog.demir.io/hands-on-with-rag-step-by-step-guide-to-integrating-retrieval-augme
nted-generation-in-llms-ac3cb075ab6f
26. https://towardsdatascience.com/how-to-build-a-semantic-search-engine-with-transformer
s-and-faiss-dcbea307a0e8
27. https://www.comet.com/site/blog/evaluating-rag-pipelines-with-ragas/
28. https://github.com/ZeusSama0001/RAG-chatbot
29. https://community.sap.com/t5/technology-blogs-by-members/vector-databases-and-embe
ddings-revolutionizing-ai-in-rag-in-llm-or-gpt/ba-p/13575985
30. https://www.linkedin.com/pulse/executive-guide-vector-databases-rag-models-david-h-de
ans-lfs6c
31. https://blog.gopenai.com/primer-on-vector-databases-and-retrieval-augmented-generation
-rag-using-langchain-pinecone-37a27fb10546
32. https://community.openai.com/t/best-vector-database-to-use-with-rag/615350
33. https://www.aporia.com/learn/best-vector-dbs-for-retrieval-augmented-generation-rag/
34. https://nanonets.com/blog/langchain/
35. https://www.techtarget.com/searchenterpriseai/definition/LangChain
36. https://www.datacamp.com/tutorial/introduction-to-lanchain-for-data-engineering-and-dat
a-applications
37. https://www.geeksforgeeks.org/introduction-to-langchain/
38. https://semaphoreci.com/blog/langchain
39. Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the
Intrinsic Dimension of Objective Landscapes. arXiv:1804.08838 [cs, stat], April 2018a.
URL http: //arxiv.org/abs/1804.08838. arXiv: 1804.08838.
40. Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic Dimensionality
Explains the Effectiveness of Language Model Fine-Tuning. arXiv:2012.13255 [cs],
December 2020. URL http://arxiv.org/abs/2012.13255.
41. Liu, Yinhan, et al. "Roberta: A robustly optimized bert pretraining approach." arXiv
preprint arXiv:1907.11692 (2019). https://arxiv.org/abs/1907.11692
42. Raffel, Colin, et al. "Exploring the limits of transfer learning with a unified text-to-text
transformer." Journal of machine learning research 21.140 (2020): 1-67.
https://arxiv.org/abs/1910.10683
43. Simple and Effective Retrieve-Edit-Rerank Text Generation (Hossain et al., ACL 2020)
44. Ligot, Dominic. (2024). Performance, Skills, Ethics, Generative AI Adoption, and the
Philippines. 10.13140/RG.2.2.14759.52643.
45. Ainslie, Joshua, et al. "Gqa: Training generalized multi-query transformer models from
multi-head checkpoints." arXiv preprint arXiv:2305.13245 (2023).
46. Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document
transformer. arXiv preprint arXiv:2004.05150, 2020.
44
47. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences
with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.
48. https://www.analyticsvidhya.com/blog/2023/10/rags-innovative-approach-to-unifying-ret
rieval-and-generation-in-nlp/
49. Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective Approaches to
Attention-based Neural Machine Translation. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing, pages 1412–1421, Lisbon, Portugal.
Association for Computational Linguistics.
50. Peng, Yifan & Yan, Ke & Sandfort, Veit & Summers, Ronald & lu, Zhiyong. (2019). A
self-attention based deep learning method for lesion attribute detection from CT reports.
1-5. 10.1109/ICHI.2019.8904668.
51. Zhang, Zhihui & Fort, Josep & Mateu, Lluis. (2023). Exploring the Potential of Artificial
Intelligence as a Tool for Architectural Design: A Perception Study Using Gaudí’sWorks.
Buildings. 13. 1863. 10.3390/buildings13071863.
52. Benedetto, Irene & Koudounas, Alkis & Vaiani, Lorenzo & Pastor, Eliana & Cagliero,
Luca & Tarasconi, Francesco & Baralis, Elena. (2024). Boosting court judgment
prediction and explanation using legal entities. Artificial Intelligence and Law. 1-36.
10.1007/s10506-024-09397-8.
53. Paudyal, Bed. (2010). Imperial and National Translations of Jang Bahadur's Visit to
Europe. South Asian Review. 31. 165-185. 10.1080/02759527.2010.11932734.
54. Pathak, Avik & Shree, Om & Agarwal, Mallika & Sarkar, Shek & Tiwary, Anupam.
(2023). Performance Analysis of LoRA Finetuning Llama-2. 1-4.
10.1109/IEMENTech60402.2023.10423400.
